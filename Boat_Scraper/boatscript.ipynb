{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('ABE_BOAT/boat_urls.csv',names=['year','url'])\n",
    "\n",
    "urlist = df.url.tolist()\n",
    "\n",
    "all_sub_urls = []\n",
    "for url in urlist: \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    try:\n",
    "        menu = soup.select(\"select.js-nav-select\")[0]\n",
    "\n",
    "        for i in range(len(menu('option'))):\n",
    "            sub_url = menu('option')[i]['data-url']\n",
    "            all_sub_urls.append(sub_url)\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        all_sub_urls.append(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import requests\n",
    "# from StringIO import StringIO\n",
    "\n",
    "#=============================================\n",
    "# This part creates a tracker that records the number of urls scraped over a period of time, it loops every 10 seconds\n",
    "tracker = []\n",
    "while True:    \n",
    "    db = pd.read_csv('boats_database.csv',low_memory=False)\n",
    "    length_completed = len(db.this_url.value_counts())\n",
    "    now = dt.now()\n",
    "    time_elapsed = now - last_time[-1] \n",
    "    left = (23889-length_completed)\n",
    "    completed_over_interval = total_left[-1] - left\n",
    "    tracker.append([time_elapsed,completed_over_interval])\n",
    "    print(\"Completed:\",length_completed,\"Left: \",left, \"time_elapsed:\",time_elapsed, \"completed_over_int:\",completed_over_interval)\n",
    "    time.sleep(10)\n",
    "    \n",
    "#=============================================\n",
    "# This part calculates the average number of urls scraped over a period of time    \n",
    "dft = pd.DataFrame(tracker,columns=['time','completed'])\n",
    "\n",
    "# tracker script was run multiple times causing the number of urls between tracker runs huge\n",
    "# so I filtered everything below 50 to not screw up the average\n",
    "dft = dft[dft.completed<50] \n",
    "dft.mean()\n",
    "\n",
    "#=============================================\n",
    "# This part finds the last url in the database and finds out how many are left, then estimates \n",
    "# the time it will take by using the averages computed from the tracker created above\n",
    "while True:\n",
    "    with open(\"boats_database.csv\", 'r') as f:\n",
    "        q = deque(f, 1)  # reads last line of the boat database\n",
    "        current_url = q[-1].split(',')[-1].split('\\n')[0] # grabs the url\n",
    "        current_index = dfurl[dfurl.url == current_url].index[0] # gets the index of current url in the url list\n",
    "        number_left = len(dfurl) - current_index # finds out how many urls are left to scrape\n",
    "        minutes_left = ((number_left/13.5)*10.543281)/60 # \n",
    "#         print(minutes_left,'Minutes Left')\n",
    "        time_done = dt.now() + timedelta(minutes=minutes_left)\n",
    "        print(time_done.hour - 12,':',time_done.minute)\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_complete = round((length_completed/23889),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:    \n",
    "    db = pd.read_csv('boats_database.csv',low_memory=False)\n",
    "    length_completed = len(db.this_url.value_counts())\n",
    "    now = dt.now()\n",
    "    time_elapsed = now - last_time[-1] \n",
    "    last_time.append(now)\n",
    "    left = (23889-length_completed)\n",
    "    completed_over_interval = total_left[-1] - left\n",
    "    tracker.append([time_elapsed,completed_over_interval])\n",
    "    total_left.append(left)\n",
    "    last_time.append(now)\n",
    "    print(\"Completed:\",length_completed,\"Left: \",left, \"time_elapsed:\",time_elapsed, \"completed_over_int:\",completed_over_interval)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year\n",
    "df['year'] = df.this_url.str.split('/').str[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['manufacturer'] = df.this_url.str.split('/').str[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.this_url.str.split('/').str[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('boat_db_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
