{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import requests\n",
    "# from StringIO import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates a tracker that records the number of urls scraped over a period of time, it loops every 10 seconds\n",
    "tracker = []\n",
    "while True:    \n",
    "    db = pd.read_csv('boats_database.csv',low_memory=False)\n",
    "    length_completed = len(db.this_url.value_counts())\n",
    "    now = dt.now()\n",
    "    time_elapsed = now - last_time[-1] \n",
    "    left = (23889-length_completed)\n",
    "    completed_over_interval = total_left[-1] - left\n",
    "    tracker.append([time_elapsed,completed_over_interval])\n",
    "    print(\"Completed:\",length_completed,\"Left: \",left, \"time_elapsed:\",time_elapsed, \"completed_over_int:\",completed_over_interval)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part calculates the average number of urls scraped over a period of time    \n",
    "dft = pd.DataFrame(tracker,columns=['time','completed'])\n",
    "# tracker script was run multiple times causing the number of urls between tracker runs huge\n",
    "# so I filtered everything below 50 to not screw up the average\n",
    "dft = dft[dft.completed<50] \n",
    "dft.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part finds the last url in the database and finds out how many are left, then estimates \n",
    "# the time it will take by using the averages computed from the tracker created above\n",
    "while True:\n",
    "    with open(\"boats_database.csv\", 'r') as f:\n",
    "        q = deque(f, 1)  # reads last line of the boat database\n",
    "        current_url = q[-1].split(',')[-1].split('\\n')[0] # grabs the url\n",
    "        current_index = dfurl[dfurl.url == current_url].index[0] # gets the index of current url in the url list\n",
    "        number_left = len(dfurl) - current_index # finds out how many urls are left to scrape\n",
    "        minutes_left = ((number_left/13.5)*10.543281)/60 # \n",
    "#         print(minutes_left,'Minutes Left')\n",
    "        time_done = dt.now() + timedelta(minutes=minutes_left)\n",
    "        print(time_done.hour - 12,':',time_done.minute)\n",
    "        time.sleep(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
